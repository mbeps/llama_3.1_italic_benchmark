{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf46e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 16:21:16 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from llama31_benchmark import Llama31BenchmarkConfig, Llama31Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a280a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face token from environment\n",
      "‚úÖ LoRA adapter found at: ./results_test\n",
      "üìã Base model resolved from adapter: DeepMount00/Llama-3.1-8b-ITA\n"
     ]
    }
   ],
   "source": [
    "config = Llama31BenchmarkConfig(\n",
    "    lora_adapter_path=\"./results_test\",\n",
    "    min_vram_gb=16.0,\n",
    "    test_file=\"./data/test.jsonl\",\n",
    "    max_eval_samples=None,\n",
    "    batch_size=4,\n",
    "    gpu_memory_utilization=0.75,         # Slightly higher\n",
    "    max_num_seqs=64,\n",
    "    max_length=4096,                     # üîß CRITICAL: Reduced from 30000 to 4096\n",
    "    tensor_parallel_size=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    top_k=-1,\n",
    "    max_new_tokens=350,\n",
    "    output_prefix=\"llama31_8b_ita_lora\",\n",
    "    system_message=\"Sei un assistente utile.\",\n",
    "    merge_adapter_in_memory=True,\n",
    "    merged_model_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc00a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "üîÑ LoRA ADAPTER SUPPORT ENABLED\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "LoRA Adapter: ./results_test\n",
      "Merge in memory: True\n",
      "Base model from adapter config: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.75\n",
      "Max concurrent sequences: 64\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita_lora_lora_results_test\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.7GB)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "config.print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137fe9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "VRAM: 23.7 GB\n",
      "‚úÖ GPU has sufficient VRAM for the model\n",
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "üîÑ LoRA ADAPTER SUPPORT ENABLED\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "LoRA Adapter: ./results_test\n",
      "Merge in memory: True\n",
      "Base model from adapter config: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.75\n",
      "Max concurrent sequences: 64\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita_lora_lora_results_test\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.7GB)\n",
      "============================================================\n",
      "üßπ Clearing GPU memory...\n",
      "GPU cache cleared\n",
      "GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB, Total: 23.7GB\n",
      "Available: 23.69GB\n",
      "‚úÖ Memory cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "benchmark = Llama31Benchmark(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efcef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK (vLLM)\n",
      "üîÑ WITH LORA ADAPTER SUPPORT\n",
      "============================================================\n",
      "Loading ITALIC dataset...\n",
      "Loaded 10000 questions\n",
      "Using all 10000 questions for evaluation\n",
      "\n",
      "Sample question:\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolarit√†: la possibilit√† di una sua vittoria si allontana sempre di pi√π\" contiene un verbo:\n",
      "Options: [{'A': 'riflessivo diretto'}, {'B': 'impersonale'}, {'C': 'passivo'}, {'D': 'intransitivo pronominale'}]\n",
      "Answer: D\n",
      "Category: orthography\n",
      "üîÑ LoRA adapter specified - will merge with base model first\n",
      "\n",
      "üîÑ Merging LoRA adapters with base model...\n",
      "Adapter path: ./results_test\n",
      "Base model: DeepMount00/Llama-3.1-8b-ITA\n",
      "üì¶ Loading LoRA adapter and base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf41e9dd5bce4e86a66d31f979e2184a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PEFT model loaded: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "üîÑ Merging LoRA adapters into base model...\n",
      "‚úÖ Adapters merged: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "üíæ Saving merged model to temporary directory: /tmp/llama31_merged_3hkbpap8\n",
      "‚úÖ Tokenizer saved with merged model\n",
      "üßπ Clearing GPU memory...\n",
      "GPU cache cleared\n",
      "GPU Memory - Allocated: 0.01GB, Reserved: 0.02GB, Total: 23.7GB\n",
      "Available: 23.67GB\n",
      "‚úÖ Memory cleanup completed!\n",
      "‚úÖ LoRA adapters successfully merged!\n",
      "üìÅ Merged model available at: /tmp/llama31_merged_3hkbpap8\n",
      "üîÑ Loading DeepMount00/Llama-3.1-8b-ITA + LoRA adapter with vLLM...\n",
      "Model path: /tmp/llama31_merged_3hkbpap8\n",
      "INFO 08-02 16:22:16 [config.py:1604] Using max model len 4096\n",
      "INFO 08-02 16:22:18 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-02 16:22:18 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 08-02 16:22:22 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-02 16:22:23 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-02 16:22:23 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/llama31_merged_3hkbpap8', speculative_config=None, tokenizer='/tmp/llama31_merged_3hkbpap8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/llama31_merged_3hkbpap8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "INFO 08-02 16:22:25 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-02 16:22:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-02 16:22:25 [gpu_model_runner.py:1843] Starting to load model /tmp/llama31_merged_3hkbpap8...\n",
      "INFO 08-02 16:22:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-02 16:22:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.65s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:08,  4.00s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:04,  4.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.07s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.43s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 16:22:39 [default_loader.py:262] Loading weights took 13.79 seconds\n",
      "INFO 08-02 16:22:39 [gpu_model_runner.py:1892] Model loading took 14.9889 GiB and 14.041007 seconds\n",
      "INFO 08-02 16:22:46 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/8b8571f609/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-02 16:22:46 [backends.py:541] Dynamo bytecode transform time: 6.26 s\n",
      "INFO 08-02 16:22:49 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-02 16:23:10 [backends.py:215] Compiling a graph for dynamic shape takes 24.15 s\n",
      "INFO 08-02 16:23:18 [monitor.py:34] torch.compile takes 30.41 s in total\n",
      "INFO 08-02 16:23:19 [gpu_worker.py:255] Available KV cache memory: 1.84 GiB\n",
      "INFO 08-02 16:23:19 [kv_cache_utils.py:833] GPU KV cache size: 15,072 tokens\n",
      "INFO 08-02 16:23:19 [kv_cache_utils.py:837] Maximum concurrency for 4,096 tokens per request: 3.68x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 16:23:20 [gpu_model_runner.py:2485] Graph capturing finished in 1 secs, took 0.56 GiB\n",
      "INFO 08-02 16:23:20 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.14 seconds\n",
      "‚úì vLLM model loaded successfully!\n",
      "‚úì LoRA adapter merged and loaded\n",
      "‚úì Base model: DeepMount00/Llama-3.1-8b-ITA\n",
      "‚úì Temperature: 0.0 (deterministic)\n",
      "GPU Memory - Allocated: 0.01GB, Reserved: 0.02GB\n",
      "\n",
      "üß™ Testing inference...\n",
      "INFO 08-02 16:23:22 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolarit√†: la possibilit√† di una sua vittoria si allontana sempre di pi√π\" contiene un verbo:\n",
      "Expected answer: 'D'\n",
      "Raw response: 'B'\n",
      "Extracted answer: 'B'\n",
      "Correct: False\n",
      "\n",
      "==================================================\n",
      "STARTING EVALUATION\n",
      "==================================================\n",
      "\n",
      "üîç Evaluating DeepMount00/Llama-3.1-8b-ITA + LoRA (results_test) on 10000 questions with batch size 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [06:44<00:00,  6.18it/s, Acc=72.7%, Questions=9964/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL RESULTS:\n",
      "Total questions: 10000\n",
      "Correct answers: 7275\n",
      "Accuracy: 0.7275 (72.75%)\n",
      "\n",
      "ZERO-SHOT RESULTS BY CATEGORY:\n",
      "------------------------------------------------------------\n",
      "Category                      Accuracy   Correct/Total\n",
      "------------------------------------------------------------\n",
      "art_history                     70.31%    689/980   \n",
      "civic_education                 77.29%    752/973   \n",
      "current_events                  82.61%     76/92    \n",
      "geography                       80.90%    792/979   \n",
      "history                         76.28%    746/978   \n",
      "lexicon                         83.76%    820/979   \n",
      "literature                      71.24%    701/984   \n",
      "morphology                      61.43%     86/140   \n",
      "orthography                     54.07%    525/971   \n",
      "synonyms_and_antonyms           89.91%    873/971   \n",
      "syntax                          52.31%    509/973   \n",
      "tourism                         72.04%    706/980   \n",
      "------------------------------------------------------------\n",
      "TOTAL                           72.75%   7275/10000 \n",
      "\n",
      "Results saved to 'llama31_8b_ita_lora_lora_results_test_rtx_results.csv'\n",
      "Summary saved to 'llama31_8b_ita_lora_lora_results_test_rtx_summary.json'\n",
      "\n",
      "üßπ Final cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W802 16:30:07.725881322 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Cleaned up temporary merged model: /tmp/llama31_merged_3hkbpap8\n",
      "Final GPU memory usage: 0.01GB\n",
      "\n",
      "üéâ BENCHMARK COMPLETED!\n",
      "============================================================\n",
      "üìä Model: DeepMount00/Llama-3.1-8b-ITA + LoRA (results_test)\n",
      "üìä Final accuracy: 0.7275 (72.75%)\n",
      "üìä Total questions evaluated: 10000\n",
      "üìä Batch size used: 4\n",
      "üìä LoRA adapter merged and evaluated\n",
      "üìä vLLM engine with efficient batching\n",
      "============================================================\n",
      "‚úÖ DeepMount00/Llama-3.1-8b-ITA + LoRA (results_test) vLLM benchmark complete! üöÄ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "results, accuracy, category_stats = benchmark.run_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b875351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7275 (72.75%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Print final results\n",
    "print(f\"Final accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97621ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions evaluated: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "print(f\"Total questions evaluated: {len(results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0858904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved with prefix: llama31_8b_ita_lora_lora_results_test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "print(f\"Results saved with prefix: {config.output_prefix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
