{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 20:09:32 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from llama31_benchmark import Llama31BenchmarkConfig, Llama31Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a280a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face token from environment\n",
      "‚úÖ LoRA adapter found at: ./model\n",
      "üìã Base model resolved from adapter: DeepMount00/Llama-3.1-8b-ITA\n"
     ]
    }
   ],
   "source": [
    "config = Llama31BenchmarkConfig(\n",
    "    lora_adapter_path=\"./model\",\n",
    "    min_vram_gb=16.0,\n",
    "    test_file=\"./data/test.jsonl\",\n",
    "    max_eval_samples=None,\n",
    "    batch_size=4,\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_num_seqs=64,\n",
    "    max_length=4096,\n",
    "    tensor_parallel_size=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    top_k=-1,\n",
    "    max_new_tokens=350,\n",
    "    output_prefix=\"llama31_8b_ita_lora\",\n",
    "    system_message=\"Sei un assistente utile.\",\n",
    "    merge_adapter_in_memory=True,\n",
    "    merged_model_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc00a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "üîÑ LoRA ADAPTER SUPPORT ENABLED\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "LoRA Adapter: ./model\n",
      "Merge in memory: True\n",
      "Base model from adapter config: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.85\n",
      "Max concurrent sequences: 64\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita_lora_lora_model\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.7GB)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fe9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "VRAM: 23.7 GB\n",
      "‚úÖ GPU has sufficient VRAM for the model\n",
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "üîÑ LoRA ADAPTER SUPPORT ENABLED\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "LoRA Adapter: ./model\n",
      "Merge in memory: True\n",
      "Base model from adapter config: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.85\n",
      "Max concurrent sequences: 64\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita_lora_lora_model\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.7GB)\n",
      "============================================================\n",
      "üßπ Clearing GPU memory...\n",
      "GPU cache cleared\n",
      "GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB, Total: 23.7GB\n",
      "Available: 23.69GB\n",
      "‚úÖ Memory cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "benchmark = Llama31Benchmark(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efcef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK (vLLM)\n",
      "üîÑ WITH LORA ADAPTER SUPPORT\n",
      "============================================================\n",
      "Loading ITALIC dataset...\n",
      "Loaded 10000 questions\n",
      "Using all 10000 questions for evaluation\n",
      "\n",
      "Sample question:\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolarit√†: la possibilit√† di una sua vittoria si allontana sempre di pi√π\" contiene un verbo:\n",
      "Options: [{'A': 'riflessivo diretto'}, {'B': 'impersonale'}, {'C': 'passivo'}, {'D': 'intransitivo pronominale'}]\n",
      "Answer: D\n",
      "Category: orthography\n",
      "üîÑ LoRA adapter specified - will merge with base model first\n",
      "\n",
      "üîÑ Merging LoRA adapters with base model...\n",
      "Adapter path: ./model\n",
      "Base model: DeepMount00/Llama-3.1-8b-ITA\n",
      "üì¶ Loading LoRA adapter and base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd4a09c065d4d85b05c56fd06272829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/923 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0b380a06834d05929b814720328cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47740053186e48b999c59d3a81f83b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca234db039b4ef4939d483bd85fff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941b11869fb04e8e8b43f123727f3281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cf28961674456a7398dde2a31432c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c50f6df515b4456b07bc8e0c367c46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c9adc80baa443190574d3a3061ea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba026650b3cf48d7a8886ec8f5a65d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PEFT model loaded: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "üîÑ Merging LoRA adapters into base model...\n",
      "‚úÖ Adapters merged: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "üíæ Saving merged model to temporary directory: /tmp/llama31_merged_a4eb2jsz\n",
      "‚úÖ Tokenizer saved with merged model\n",
      "üßπ Clearing GPU memory...\n",
      "GPU cache cleared\n",
      "GPU Memory - Allocated: 0.01GB, Reserved: 0.02GB, Total: 23.7GB\n",
      "Available: 23.67GB\n",
      "‚úÖ Memory cleanup completed!\n",
      "‚úÖ LoRA adapters successfully merged!\n",
      "üìÅ Merged model available at: /tmp/llama31_merged_a4eb2jsz\n",
      "üîÑ Loading DeepMount00/Llama-3.1-8b-ITA + LoRA adapter with vLLM...\n",
      "Model path: /tmp/llama31_merged_a4eb2jsz\n",
      "INFO 08-14 20:12:57 [config.py:1604] Using max model len 4096\n",
      "INFO 08-14 20:12:59 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-14 20:13:00 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 08-14 20:13:05 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-14 20:13:06 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-14 20:13:06 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/llama31_merged_a4eb2jsz', speculative_config=None, tokenizer='/tmp/llama31_merged_a4eb2jsz', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/llama31_merged_a4eb2jsz, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "INFO 08-14 20:13:09 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-14 20:13:09 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-14 20:13:09 [gpu_model_runner.py:1843] Starting to load model /tmp/llama31_merged_a4eb2jsz...\n",
      "INFO 08-14 20:13:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-14 20:13:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 20:13:12 [default_loader.py:262] Loading weights took 2.85 seconds\n",
      "INFO 08-14 20:13:13 [gpu_model_runner.py:1892] Model loading took 14.9889 GiB and 3.226365 seconds\n",
      "INFO 08-14 20:13:22 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/16bc54da61/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-14 20:13:22 [backends.py:541] Dynamo bytecode transform time: 8.94 s\n",
      "INFO 08-14 20:13:26 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-14 20:13:57 [backends.py:215] Compiling a graph for dynamic shape takes 34.41 s\n",
      "INFO 08-14 20:14:05 [monitor.py:34] torch.compile takes 43.35 s in total\n",
      "INFO 08-14 20:14:06 [gpu_worker.py:255] Available KV cache memory: 4.21 GiB\n",
      "INFO 08-14 20:14:06 [kv_cache_utils.py:833] GPU KV cache size: 34,480 tokens\n",
      "INFO 08-14 20:14:06 [kv_cache_utils.py:837] Maximum concurrency for 4,096 tokens per request: 8.42x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 20:14:08 [gpu_model_runner.py:2485] Graph capturing finished in 1 secs, took 0.56 GiB\n",
      "INFO 08-14 20:14:08 [core.py:193] init engine (profile, create kv cache, warmup model) took 54.97 seconds\n",
      "‚úì vLLM model loaded successfully!\n",
      "‚úì LoRA adapter merged and loaded\n",
      "‚úì Base model: DeepMount00/Llama-3.1-8b-ITA\n",
      "‚úì Temperature: 0.0 (deterministic)\n",
      "GPU Memory - Allocated: 0.01GB, Reserved: 0.02GB\n",
      "\n",
      "üß™ Testing inference...\n",
      "INFO 08-14 20:14:09 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolarit√†: la possibilit√† di una sua vittoria si allontana sempre di pi√π\" contiene un verbo:\n",
      "Expected answer: 'D'\n",
      "Raw response: 'C'\n",
      "Extracted answer: 'C'\n",
      "Correct: False\n",
      "\n",
      "==================================================\n",
      "STARTING EVALUATION\n",
      "==================================================\n",
      "\n",
      "üîç Evaluating DeepMount00/Llama-3.1-8b-ITA + LoRA (model) on 10000 questions with batch size 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [06:51<00:00,  6.07it/s, Acc=72.0%, Questions=9964/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL RESULTS:\n",
      "Total questions: 10000\n",
      "Correct answers: 7196\n",
      "Accuracy: 0.7196 (71.96%)\n",
      "\n",
      "ZERO-SHOT RESULTS BY CATEGORY:\n",
      "------------------------------------------------------------\n",
      "Category                      Accuracy   Correct/Total\n",
      "------------------------------------------------------------\n",
      "art_history                     70.00%    686/980   \n",
      "civic_education                 75.44%    734/973   \n",
      "current_events                  78.26%     72/92    \n",
      "geography                       80.69%    790/979   \n",
      "history                         75.97%    743/978   \n",
      "lexicon                         81.61%    799/979   \n",
      "literature                      68.60%    675/984   \n",
      "morphology                      57.14%     80/140   \n",
      "orthography                     56.95%    553/971   \n",
      "synonyms_and_antonyms           85.68%    832/971   \n",
      "syntax                          54.16%    527/973   \n",
      "tourism                         71.94%    705/980   \n",
      "------------------------------------------------------------\n",
      "TOTAL                           71.96%   7196/10000 \n",
      "\n",
      "Results saved to 'llama31_8b_ita_lora_lora_model_rtx_results.csv'\n",
      "Summary saved to 'llama31_8b_ita_lora_lora_model_rtx_summary.json'\n",
      "\n",
      "üßπ Final cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W814 20:21:01.877272370 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Cleaned up temporary merged model: /tmp/llama31_merged_a4eb2jsz\n",
      "Final GPU memory usage: 0.01GB\n",
      "\n",
      "üéâ BENCHMARK COMPLETED!\n",
      "============================================================\n",
      "üìä Model: DeepMount00/Llama-3.1-8b-ITA + LoRA (model)\n",
      "üìä Final accuracy: 0.7196 (71.96%)\n",
      "üìä Total questions evaluated: 10000\n",
      "üìä Batch size used: 4\n",
      "üìä LoRA adapter merged and evaluated\n",
      "üìä vLLM engine with efficient batching\n",
      "============================================================\n",
      "‚úÖ DeepMount00/Llama-3.1-8b-ITA + LoRA (model) vLLM benchmark complete! üöÄ\n"
     ]
    }
   ],
   "source": [
    "results, accuracy, category_stats = benchmark.run_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7196 (71.96%)\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(f\"Final accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97621ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions evaluated: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total questions evaluated: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0858904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved with prefix: llama31_8b_ita_lora_lora_model\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results saved with prefix: {config.output_prefix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
