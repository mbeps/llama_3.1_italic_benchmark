{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd11af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 18:16:23 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from llama31_benchmark import Llama31BenchmarkConfig, Llama31Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d56acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face token from environment\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Total VRAM: 23.6 GB\n",
      "Model requires: 16.0 GB\n",
      "GPU memory utilization: 0.9\n",
      "Auto-detected batch size: 4\n"
     ]
    }
   ],
   "source": [
    "config = Llama31BenchmarkConfig(\n",
    "    model_name=\"DeepMount00/Llama-3.1-8b-ITA\",\n",
    "    min_vram_gb=16.0,                    # Minimum VRAM required\n",
    "    test_file=\"./data/test.jsonl\",              # Your test dataset\n",
    "    max_eval_samples=None,               # Use full dataset\n",
    "    batch_size=None,                     # Auto-detect optimal batch size\n",
    "    gpu_memory_utilization=0.9,          # Use 90% of GPU memory\n",
    "    max_num_seqs=256,                    # Max concurrent sequences\n",
    "    tensor_parallel_size=1,              # Single GPU\n",
    "    temperature=0.0,                     # Deterministic generation\n",
    "    top_p=1.0,                           # Consider all tokens (vLLM default)\n",
    "    top_k=-1,                            # Consider all tokens (vLLM default)\n",
    "    max_new_tokens=350,                  # Max tokens to generate\n",
    "    output_prefix=\"llama31_8b_ita\",      # Output file prefix\n",
    "    system_message=\"Sei un assistente utile.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c5b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.9\n",
      "Max concurrent sequences: 256\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.6GB)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f256665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "VRAM: 23.6 GB\n",
      "âœ… GPU has sufficient VRAM for the model\n",
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK CONFIGURATION (vLLM)\n",
      "============================================================\n",
      "Model: DeepMount00/Llama-3.1-8b-ITA\n",
      "Test file: ./data/test.jsonl\n",
      "Max samples: All\n",
      "Batch size: 4\n",
      "GPU memory utilization: 0.9\n",
      "Max concurrent sequences: 256\n",
      "Temperature: 0.0\n",
      "Output prefix: llama31_8b_ita\n",
      "System message: Sei un assistente utile.\n",
      "GPU: NVIDIA GeForce RTX 3090 (23.6GB)\n",
      "============================================================\n",
      "ðŸ§¹ Clearing GPU memory...\n",
      "GPU cache cleared\n",
      "GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB, Total: 23.6GB\n",
      "Available: 23.57GB\n",
      "âœ… Memory cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "benchmark = Llama31Benchmark(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a58e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLAMA 3.1 BENCHMARK (vLLM)\n",
      "============================================================\n",
      "Loading ITALIC dataset...\n",
      "Loaded 10000 questions\n",
      "Using all 10000 questions for evaluation\n",
      "\n",
      "Sample question:\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolaritÃ : la possibilitÃ  di una sua vittoria si allontana sempre di piÃ¹\" contiene un verbo:\n",
      "Options: [{'A': 'riflessivo diretto'}, {'B': 'impersonale'}, {'C': 'passivo'}, {'D': 'intransitivo pronominale'}]\n",
      "Answer: D\n",
      "Category: orthography\n",
      "ðŸ”„ Loading DeepMount00/Llama-3.1-8b-ITA with vLLM...\n",
      "INFO 07-30 18:16:32 [config.py:1604] Using max model len 30000\n",
      "INFO 07-30 18:16:35 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 07-30 18:16:36 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 07-30 18:16:40 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 07-30 18:16:41 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 07-30 18:16:41 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='DeepMount00/Llama-3.1-8b-ITA', speculative_config=None, tokenizer='DeepMount00/Llama-3.1-8b-ITA', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=30000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=DeepMount00/Llama-3.1-8b-ITA, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-30 18:16:43 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-30 18:16:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-30 18:16:43 [gpu_model_runner.py:1843] Starting to load model DeepMount00/Llama-3.1-8b-ITA...\n",
      "INFO 07-30 18:16:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 07-30 18:16:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-30 18:16:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  9.97it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.29it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 18:16:45 [default_loader.py:262] Loading weights took 1.71 seconds\n",
      "INFO 07-30 18:16:46 [gpu_model_runner.py:1892] Model loading took 14.9889 GiB and 2.421712 seconds\n",
      "INFO 07-30 18:16:52 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/1b1b72c099/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-30 18:16:52 [backends.py:541] Dynamo bytecode transform time: 6.14 s\n",
      "INFO 07-30 18:16:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.445 s\n",
      "INFO 07-30 18:16:58 [monitor.py:34] torch.compile takes 6.14 s in total\n",
      "INFO 07-30 18:17:00 [gpu_worker.py:255] Available KV cache memory: 4.97 GiB\n",
      "INFO 07-30 18:17:00 [kv_cache_utils.py:833] GPU KV cache size: 40,720 tokens\n",
      "INFO 07-30 18:17:00 [kv_cache_utils.py:837] Maximum concurrency for 30,000 tokens per request: 1.36x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:05<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 18:17:07 [gpu_model_runner.py:2485] Graph capturing finished in 6 secs, took 0.53 GiB\n",
      "INFO 07-30 18:17:07 [core.py:193] init engine (profile, create kv cache, warmup model) took 20.75 seconds\n",
      "âœ“ vLLM model loaded successfully!\n",
      "âœ“ Temperature: 0.0 (deterministic)\n",
      "GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB\n",
      "\n",
      "ðŸ§ª Testing inference...\n",
      "INFO 07-30 18:17:09 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "Question: La frase \"Secondo recenti sondaggi il candidato repubblicano gode di scarsissima popolaritÃ : la possibilitÃ  di una sua vittoria si allontana sempre di piÃ¹\" contiene un verbo:\n",
      "Expected answer: 'D'\n",
      "Raw response: 'B'\n",
      "Extracted answer: 'B'\n",
      "Correct: False\n",
      "\n",
      "==================================================\n",
      "STARTING EVALUATION\n",
      "==================================================\n",
      "\n",
      "ðŸ” Evaluating DeepMount00/Llama-3.1-8b-ITA on 10000 questions with batch size 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [07:19<00:00,  5.69it/s, Acc=70.3%, Questions=9964/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL RESULTS:\n",
      "Total questions: 10000\n",
      "Correct answers: 7029\n",
      "Accuracy: 0.7029 (70.29%)\n",
      "\n",
      "ZERO-SHOT RESULTS BY CATEGORY:\n",
      "------------------------------------------------------------\n",
      "Category                      Accuracy   Correct/Total\n",
      "------------------------------------------------------------\n",
      "art_history                     68.88%    675/980   \n",
      "civic_education                 70.61%    687/973   \n",
      "current_events                  83.70%     77/92    \n",
      "geography                       79.16%    775/979   \n",
      "history                         75.97%    743/978   \n",
      "lexicon                         81.31%    796/979   \n",
      "literature                      67.07%    660/984   \n",
      "morphology                      55.71%     78/140   \n",
      "orthography                     51.18%    497/971   \n",
      "synonyms_and_antonyms           85.27%    828/971   \n",
      "syntax                          52.72%    513/973   \n",
      "tourism                         71.43%    700/980   \n",
      "------------------------------------------------------------\n",
      "TOTAL                           70.29%   7029/10000 \n",
      "\n",
      "Results saved to 'llama31_8b_ita_rtx_results.csv'\n",
      "Summary saved to 'llama31_8b_ita_rtx_summary.json'\n",
      "\n",
      "ðŸ§¹ Final GPU cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W730 18:24:29.004586129 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GPU memory usage: 0.00GB\n",
      "\n",
      "ðŸŽ‰ BENCHMARK COMPLETED!\n",
      "============================================================\n",
      "ðŸ“Š Final accuracy: 0.7029 (70.29%)\n",
      "ðŸ“Š Total questions evaluated: 10000\n",
      "ðŸ“Š Batch size used: 4\n",
      "ðŸ“Š vLLM engine with efficient batching\n",
      "============================================================\n",
      "âœ… DeepMount00/Llama-3.1-8b-ITA vLLM benchmark complete! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "results, accuracy, category_stats = benchmark.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4739782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7029 (70.29%)\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(f\"Final accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66913bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions evaluated: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total questions evaluated: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68fbd8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved with prefix: llama31_8b_ita\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results saved with prefix: {config.output_prefix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
